# 适合新手小白！本人学习CNN、RNN、LSTM的笔记
## 基本概念：
时间序列数据：在不同时间点上收集到的同类型的数据，这类数据反映了某变量随时间的变化情况。

## CNN
处理具有多维结构的数据而设计的深度学习模型,比如二位图像数据。
卷积层利用卷积核在数据上滑动进行卷积操作，提取局部特征，不同的卷积核可以捕捉到不同的特征模式，如边缘、纹理等；
池化层则对特征图进行下采样，减少数据量的同时保留重要特征，常见的池化方法有最大池化和平均池化；
最后通过全连接层将提取到的特征进行整合，用于分类或回归等任务.


处理序列型数据的网络，一定是在时间上递归的。

## RNN：Recurrent Neural Networks，循环神经网络
### 原理
时间上进行线性递归的神经网络，用于处理序列数据。
类似于一阶微分方程组：状态空间表达式。
![rnn](rnn.png)
![rnn_strc](rnn_strc.png)

### 缺点：
计算梯度时，t时刻的导数会传播到t-1，t-2，… ，1时刻，这样就有了连乘的系数，连乘带来了两个问题：梯度爆炸和消失。
    由于每个序列的计算都是递归乘积的，导致权重矩阵的多次连乘。
    当状态计算的权重矩阵whh的特征值<1时，连乘会趋于0；>1时，连乘会爆炸。
在前向过程中，开始时刻的输入对后面时刻的影响越来越小，这就是长距离依赖问题。



## LSTM
### 介绍
为解决RNN在处理长序列数据时容易出现的梯度消失和梯度爆炸问题而设计，通过引入输入门、遗忘门和输出门等特殊结构，能够更好地控制信息的流动和长期依赖关系的捕捉。
![lstm](lstm.png)
### 原理
通过“门”结构来删除或者增加信息到细胞状态cell state。
这个cell state承载着之前所有状态的信息，每到新的时刻，就有相应的操作来决定舍弃什么旧信息以及添加什么新信息。
ct作为信息的长期记忆载体,通过遗忘门和输入门的控制，能够灵活地决定保留哪些旧信息和添加哪些新信息，以及梯度裁剪方法，避免了rnn问题。
这个状态与隐藏层状态h不同，它的更新是缓慢的，而隐藏层状态h的更新是迅速的。
所以h快，c慢，所以叫长短期时间记忆。
![lstm_yl](lstm_yl.png)


输入门决定了当前输入有多少信息可以进入细胞状态；
遗忘门决定了上一时刻的细胞状态有多少信息需要被遗忘；
输出门则控制细胞状态中的信息如何输出到当前的隐藏状态和输出。
这些门控机制使得LSTM能够选择性地记住和遗忘信息，从而更有效地处理长序列数据。