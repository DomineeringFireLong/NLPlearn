# 谈一谈对transformer的理解

首先说一下transformer的特点：
1.采用位置编码将序列的顺序信息融入到输入的嵌入向量中，避免了传统seq2seq网络中的递归循环，使得模型计算可以并行化。
2.自注意力机制，可以捕捉输入序列或者预测序列的任意词元之间的关系，打破了距离依赖的局限性。
3.注意力机制，捕捉输入序列的编码(n,dm)和每一个解码器对输出序列的自注意力运算后输出的关系，丰富了输入序列的编码信息。
    传统的基于LSTM的Seq2Seq，编码器的输出通常是一个上下文向量(1,dm)，而transformer是(n,dm),输出是一个序列的上下文向量,保留了完整的源序列信息.
4.掩码技术，控制了注意力权重的计算。
5.位置编码、掩码技术、注意力机制，实现了并行计算的能力


本质上是一种基于encoder-decoder架构，编码器对输入序列的编码信息更全面，解码器不仅关注输出序列内部关系，还关注输入编码和输出序列的关系，并且可以并行化计算的模型。